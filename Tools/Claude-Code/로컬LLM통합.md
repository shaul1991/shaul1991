# ğŸ  í™ˆ ì„œë²„ ë¡œì»¬ LLM + Claude Code í†µí•© ê°€ì´ë“œ

## ğŸ“– ê°œìš”

í™ˆ ì„œë²„ì—ì„œ ë¡œì»¬ LLMì„ í˜¸ìŠ¤íŒ…í•˜ê³  Claude Codeì™€ ì—°ë™í•˜ëŠ” ì™„ì „ ê°€ì´ë“œì…ë‹ˆë‹¤.

**ëª©í‘œ**: Claude Codeë¥¼ ì‚¬ìš©í•˜ë©´ì„œ í™ˆ ì„œë²„ì˜ ë¡œì»¬ LLMì„ í™œìš©í•˜ì—¬ ë¹„ìš© ì ˆê° ë° ì„±ëŠ¥ ìµœì í™”

---

## ğŸ–¥ï¸ í™ˆ ì„œë²„ ì‚¬ì–‘

í˜„ì¬ í™ˆ ì„œë²„ ì‚¬ì–‘:

| í•­ëª© | ì‚¬ì–‘ | LLM ì í•©ë„ |
|------|------|-----------|
| CPU | AMD Ryzen 5 2400G (8 threads) @ 3.600GHz | âœ… ì–‘í˜¸ |
| GPU | AMD Radeon RX 580 8GB | âœ… 7B-13B ëª¨ë¸ ìµœì  |
| RAM | 32GB | âœ… ì¶©ë¶„ |
| OS | Ubuntu 24.04 | âœ… ì™„ë²½ ì§€ì› |

**ê²°ë¡ **: 7B-13B í¬ê¸°ì˜ ì½”ë”© íŠ¹í™” LLM ì‹¤í–‰ì— ìµœì í™”ëœ ì‚¬ì–‘ì…ë‹ˆë‹¤.

---

## ğŸ¯ ì¶”ì²œ ëª¨ë¸ (RX 580 8GB ê¸°ì¤€)

### Tier 1: ìµœì  ì„±ëŠ¥ (ê¶Œì¥)

| ëª¨ë¸ | í¬ê¸° | VRAM | ì†ë„ | ìš©ë„ |
|------|------|------|------|------|
| **DeepSeek-Coder 6.7B** | 6.7B | ~7GB | âš¡âš¡âš¡ ë¹ ë¦„ | ì½”ë”© íŠ¹í™” |
| **CodeLlama 7B** | 7B | ~7.5GB | âš¡âš¡âš¡ ë¹ ë¦„ | ì½”ë“œ ìƒì„± |
| **Mistral 7B** | 7B | ~7.5GB | âš¡âš¡âš¡ ë¹ ë¦„ | ë²”ìš© |
| **Llama 3.1 8B** | 8B | ~8GB | âš¡âš¡ ë³´í†µ | ë²”ìš©, ìµœì‹  |

**ì˜ˆìƒ ì†ë„**: 20-40 tokens/sec

### Tier 2: 4-bit ì–‘ìí™”

| ëª¨ë¸ | í¬ê¸° | VRAM | ì†ë„ | ìš©ë„ |
|------|------|------|------|------|
| **DeepSeek-Coder 33B (4-bit)** | 33B | ~8GB | âš¡ ëŠë¦¼ | ê³ ê¸‰ ì½”ë”© |
| **Llama 2 13B (4-bit)** | 13B | ~8GB | âš¡âš¡ ë³´í†µ | ë²”ìš© |

**ì˜ˆìƒ ì†ë„**: 10-20 tokens/sec

---

## ğŸš€ ë°©ë²• 1: Ollama + Claude Code Router (ê¶Œì¥)

ê°€ì¥ ê°„ë‹¨í•˜ê³  íš¨ê³¼ì ì¸ ë°©ë²•ì…ë‹ˆë‹¤.

### Step 1: í™ˆ ì„œë²„ì— Ollama ì„¤ì¹˜

```bash
# Ubuntu 24.04ì—ì„œ Ollama ì„¤ì¹˜
curl -fsSL https://ollama.com/install.sh | sh

# ì„¤ì¹˜ í™•ì¸
ollama --version
```

### Step 2: AMD GPU ì§€ì› ì„¤ì •

```bash
# ROCm ì„¤ì¹˜ (AMD GPU ì§€ì›)
sudo apt update
sudo apt install rocm-hip-sdk -y

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
echo 'export HSA_OVERRIDE_GFX_VERSION=8.0.3' >> ~/.bashrc
source ~/.bashrc

# Ollama ì„œë¹„ìŠ¤ ì¬ì‹œì‘
sudo systemctl restart ollama
```

### Step 3: ì½”ë”© íŠ¹í™” ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

```bash
# DeepSeek-Coder 6.7B (ê°€ì¥ ì¶”ì²œ)
ollama pull deepseek-coder:6.7b

# CodeLlama 7B (ëŒ€ì•ˆ)
ollama pull codellama:7b

# Llama 3.1 8B (ìµœì‹ )
ollama pull llama3.1:8b

# ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ í™•ì¸
ollama list
```

### Step 4: ëª¨ë¸ í…ŒìŠ¤íŠ¸

```bash
# ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸
ollama run deepseek-coder:6.7b

# í”„ë¡¬í”„íŠ¸ ì˜ˆì‹œ:
# "Write a Python function to calculate fibonacci numbers"
```

### Step 5: Claude Code Router ì„¤ì¹˜

ë¡œì»¬ ë¨¸ì‹ (Claude Codeë¥¼ ì‹¤í–‰í•˜ëŠ” ê³³)ì—ì„œ:

```bash
# npxë¡œ ì§ì ‘ ì‹¤í–‰ (ê¶Œì¥)
npx claude-code-router

# ë˜ëŠ” ê¸€ë¡œë²Œ ì„¤ì¹˜
npm install -g claude-code-router
claude-code-router
```

### Step 6: Claude Code Router ì„¤ì •

```bash
# ì„¤ì • íŒŒì¼ ìƒì„±
cat > ~/.claude-code-router/config.json << 'EOF'
{
  "providers": {
    "ollama": {
      "baseUrl": "http://í™ˆì„œë²„IP:11434",
      "models": {
        "deepseek-coder": "deepseek-coder:6.7b",
        "codellama": "codellama:7b",
        "llama3": "llama3.1:8b"
      },
      "default": "deepseek-coder"
    }
  },
  "routing": {
    "claude-opus-4-5": "ollama:deepseek-coder",
    "claude-sonnet-4-5": "ollama:deepseek-coder"
  },
  "port": 3000
}
EOF

# í™ˆì„œë²„IPë¥¼ ì‹¤ì œ IPë¡œ ë³€ê²½í•˜ì„¸ìš” (ì˜ˆ: 192.168.1.100)
```

### Step 7: Claude Code ì„¤ì •

Claude Codeì˜ API ì—”ë“œí¬ì¸íŠ¸ë¥¼ ë³€ê²½:

```bash
# Claude Code ì„¤ì •
export ANTHROPIC_API_BASE=http://localhost:3000
export ANTHROPIC_API_KEY=dummy-key  # RouterëŠ” API í‚¤ í•„ìš” ì—†ìŒ

# Claude Code ì‹¤í–‰
claude-code
```

---

## ğŸ”§ ë°©ë²• 2: MCP ì„œë²„ë¡œ í™ˆ ì„œë²„ ë„êµ¬ ì—°ë™

ë” ìœ ì—°í•œ ë°©ë²•: í™ˆ ì„œë²„ë¥¼ MCP ì„œë²„ë¡œ ë…¸ì¶œí•˜ì—¬ Claude Codeì—ì„œ ë„êµ¬ë¡œ ì‚¬ìš©

### Step 1: í™ˆ ì„œë²„ì— MCP ì„œë²„ ìƒì„±

```javascript
// ~/mcp-servers/home-llm-server.js
import { Server } from "@modelcontextprotocol/sdk/server/index.js";
import { StdioServerTransport } from "@modelcontextprotocol/sdk/server/stdio.js";
import {
  CallToolRequestSchema,
  ListToolsRequestSchema,
} from "@modelcontextprotocol/sdk/types.js";
import Anthropic from "@anthropic-ai/sdk";

const server = new Server(
  {
    name: "home-llm-server",
    version: "1.0.0",
  },
  {
    capabilities: {
      tools: {},
    },
  }
);

// Ollama í˜¸ì¶œ í•¨ìˆ˜
async function callOllama(prompt, model = "deepseek-coder:6.7b") {
  const response = await fetch("http://localhost:11434/api/generate", {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
      model,
      prompt,
      stream: false,
    }),
  });
  const data = await response.json();
  return data.response;
}

// ë„êµ¬ ë“±ë¡
server.setRequestHandler(ListToolsRequestSchema, async () => {
  return {
    tools: [
      {
        name: "ask_local_llm",
        description: "í™ˆ ì„œë²„ì˜ ë¡œì»¬ LLMì—ê²Œ ì§ˆë¬¸í•˜ê¸° (DeepSeek-Coder 6.7B)",
        inputSchema: {
          type: "object",
          properties: {
            prompt: {
              type: "string",
              description: "LLMì—ê²Œ ë³´ë‚¼ í”„ë¡¬í”„íŠ¸",
            },
            model: {
              type: "string",
              description: "ì‚¬ìš©í•  ëª¨ë¸ (ê¸°ë³¸: deepseek-coder:6.7b)",
              enum: ["deepseek-coder:6.7b", "codellama:7b", "llama3.1:8b"],
            },
          },
          required: ["prompt"],
        },
      },
    ],
  };
});

// ë„êµ¬ ì‹¤í–‰
server.setRequestHandler(CallToolRequestSchema, async (request) => {
  if (request.params.name === "ask_local_llm") {
    const { prompt, model } = request.params.arguments;
    const response = await callOllama(prompt, model);
    return {
      content: [{ type: "text", text: response }],
    };
  }
  throw new Error("Unknown tool");
});

// ì„œë²„ ì‹œì‘
const transport = new StdioServerTransport();
await server.connect(transport);
```

### Step 2: Claude Codeì—ì„œ MCP ì„œë²„ ì„¤ì •

```json
// ~/.claude/mcp.json
{
  "mcpServers": {
    "home-llm": {
      "command": "node",
      "args": ["/home/user/mcp-servers/home-llm-server.js"],
      "scope": "local"
    }
  }
}
```

### Step 3: Claude Codeì—ì„œ ì‚¬ìš©

ì´ì œ Claude Codeì—ì„œ `/mcp` ëª…ë ¹ì–´ë¡œ í™ˆ ì„œë²„ì˜ LLMì„ ë„êµ¬ë¡œ ì‚¬ìš© ê°€ëŠ¥:

```
User: /mcp ask_local_llm "Write a Python function to parse JSON"
```

---

## ğŸ® ë°©ë²• 3: LiteLLM ê²Œì´íŠ¸ì›¨ì´ (ê³ ê¸‰)

ì—¬ëŸ¬ LLM ì œê³µìë¥¼ í†µí•© ê´€ë¦¬í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

### Step 1: í™ˆ ì„œë²„ì— LiteLLM ì„¤ì¹˜

```bash
# Python ê°€ìƒí™˜ê²½ ìƒì„±
python3 -m venv ~/litellm-env
source ~/litellm-env/bin/activate

# LiteLLM ì„¤ì¹˜
pip install 'litellm[proxy]'
```

### Step 2: LiteLLM ì„¤ì •

```yaml
# ~/litellm-config.yaml
model_list:
  - model_name: deepseek-coder
    litellm_params:
      model: ollama/deepseek-coder:6.7b
      api_base: http://localhost:11434

  - model_name: codellama
    litellm_params:
      model: ollama/codellama:7b
      api_base: http://localhost:11434

  - model_name: claude-sonnet
    litellm_params:
      model: claude-sonnet-4-5
      api_key: ${ANTHROPIC_API_KEY}

general_settings:
  master_key: your-secret-key

litellm_settings:
  drop_params: true
  set_verbose: true
```

### Step 3: LiteLLM í”„ë¡ì‹œ ì‹¤í–‰

```bash
# ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
litellm --config ~/litellm-config.yaml --port 8000 &

# systemd ì„œë¹„ìŠ¤ë¡œ ë“±ë¡ (ì„ íƒì‚¬í•­)
sudo tee /etc/systemd/system/litellm.service << 'EOF'
[Unit]
Description=LiteLLM Proxy
After=network.target

[Service]
Type=simple
User=user
WorkingDirectory=/home/user
ExecStart=/home/user/litellm-env/bin/litellm --config /home/user/litellm-config.yaml --port 8000
Restart=on-failure

[Install]
WantedBy=multi-user.target
EOF

sudo systemctl enable litellm
sudo systemctl start litellm
```

### Step 4: Claude Code ì„¤ì •

```bash
# OpenAI í˜¸í™˜ APIë¡œ ì‚¬ìš©
export ANTHROPIC_API_BASE=http://í™ˆì„œë²„IP:8000
export ANTHROPIC_API_KEY=your-secret-key

# Claude Code ì‹¤í–‰
claude-code
```

---

## ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (RX 580 8GB)

ì‹¤ì œ í…ŒìŠ¤íŠ¸ ê²°ê³¼ (ì˜ˆìƒ):

| ëª¨ë¸ | VRAM ì‚¬ìš© | í† í°/ì´ˆ | ì§€ì—°ì‹œê°„ (ì²« í† í°) | í’ˆì§ˆ |
|------|----------|--------|------------------|------|
| DeepSeek-Coder 6.7B | 7.2GB | 25-35 | 0.5ì´ˆ | â­â­â­â­â­ |
| CodeLlama 7B | 7.5GB | 20-30 | 0.6ì´ˆ | â­â­â­â­ |
| Llama 3.1 8B | 8.0GB | 18-25 | 0.7ì´ˆ | â­â­â­â­â­ |
| DeepSeek-Coder 33B (4-bit) | 8.0GB | 8-12 | 1.2ì´ˆ | â­â­â­â­â­â­ |

**ì¶”ì²œ**: DeepSeek-Coder 6.7Bê°€ ì†ë„ì™€ í’ˆì§ˆì˜ ìµœì  ê· í˜•ì 

---

## ğŸ’¡ ì‹¤ì „ í™œìš© íŒ

### 1. í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ (ë¹„ìš© ìµœì í™”)

Claude Code Routerë¥¼ í™œìš©í•œ ìŠ¤ë§ˆíŠ¸ ë¼ìš°íŒ…:

```json
{
  "routing": {
    "simple-tasks": "ollama:deepseek-coder",
    "complex-tasks": "anthropic:claude-opus-4-5"
  },
  "rules": {
    "ifPromptLength": {
      "lessThan": 500,
      "route": "ollama:deepseek-coder"
    },
    "ifPromptContains": {
      "keywords": ["complex", "architecture", "refactor"],
      "route": "anthropic:claude-opus-4-5"
    }
  }
}
```

**ì „ëµ**:
- âœ… ê°„ë‹¨í•œ ì½”ë“œ ìƒì„± â†’ ë¡œì»¬ LLM (ë¬´ë£Œ)
- âœ… ë³µì¡í•œ ì•„í‚¤í…ì²˜ ì„¤ê³„ â†’ Claude Opus (ìœ ë£Œ)
- ğŸ’° **ë¹„ìš© ì ˆê° ì˜ˆìƒ**: 70-80%

### 2. ëª¨ë¸ë³„ ìµœì  ìš©ë„

| ì‘ì—… ìœ í˜• | ì¶”ì²œ ëª¨ë¸ | ì´ìœ  |
|---------|---------|------|
| ì½”ë“œ ìë™ì™„ì„± | DeepSeek-Coder 6.7B | âš¡ ë¹ ë¥¸ ì‘ë‹µ |
| ë²„ê·¸ ìˆ˜ì • | CodeLlama 7B | ğŸ” ì½”ë“œ ì´í•´ë ¥ |
| ë¬¸ì„œ ìƒì„± | Llama 3.1 8B | ğŸ“ ìì—°ì–´ í’ˆì§ˆ |
| ë¦¬íŒ©í† ë§ | DeepSeek-Coder 33B (4-bit) | ğŸ§  ê³ ê¸‰ ì¶”ë¡  |
| ì•„í‚¤í…ì²˜ ì„¤ê³„ | Claude Opus 4.5 | ğŸ¯ ìµœê³  ì„±ëŠ¥ |

### 3. GPU ë©”ëª¨ë¦¬ ìµœì í™”

```bash
# Ollama í™˜ê²½ ë³€ìˆ˜ë¡œ ë©”ëª¨ë¦¬ ì œì–´
export OLLAMA_MAX_LOADED_MODELS=1  # ë™ì‹œ ë¡œë“œ ëª¨ë¸ ìˆ˜ ì œí•œ
export OLLAMA_NUM_PARALLEL=2       # ë³‘ë ¬ ìš”ì²­ ìˆ˜
export OLLAMA_FLASH_ATTENTION=1    # Flash Attention í™œì„±í™”

# Ollama ì¬ì‹œì‘
sudo systemctl restart ollama
```

### 4. ë„¤íŠ¸ì›Œí¬ ìµœì í™” (ì›ê²© ì ‘ì† ì‹œ)

```bash
# í™ˆ ì„œë²„ì—ì„œ Ollama APIë¥¼ ì™¸ë¶€ ë…¸ì¶œ
sudo ufw allow 11434/tcp

# Ollama ì„œë¹„ìŠ¤ ì„¤ì • ë³€ê²½
sudo systemctl edit ollama

# ë‹¤ìŒ ë‚´ìš© ì¶”ê°€:
[Service]
Environment="OLLAMA_HOST=0.0.0.0:11434"

# ì¬ì‹œì‘
sudo systemctl daemon-reload
sudo systemctl restart ollama
```

**ë³´ì•ˆ ì£¼ì˜**: ì™¸ë¶€ ë…¸ì¶œ ì‹œ ë°˜ë“œì‹œ Caddy/Nginxë¡œ ë¦¬ë²„ìŠ¤ í”„ë¡ì‹œ + ì¸ì¦ ì„¤ì •

---

## âš ï¸ ì£¼ì˜ì‚¬í•­ ë° ì œí•œì‚¬í•­

### í˜„ì¬ ì œí•œì‚¬í•­

1. **Claude Code ìì²´ëŠ” ë¡œì»¬ LLM ë¯¸ì§€ì›**
   - âŒ Claude Code CLIê°€ ì§ì ‘ Ollama ì‚¬ìš© ë¶ˆê°€
   - âœ… í”„ë¡ì‹œ(Router, LiteLLM)ë¥¼ í†µí•œ ìš°íšŒ í•„ìš”

2. **ì„±ëŠ¥ ì°¨ì´**
   - ğŸŒ ë¡œì»¬ 7B ëª¨ë¸ < Claude Sonnet 4.5
   - ğŸ’° í•˜ì§€ë§Œ ë¹„ìš©ì€ ë¬´ë£Œ vs ìœ ë£Œ

3. **AMD GPU í˜¸í™˜ì„±**
   - âš ï¸ ROCm ë²„ì „ í™•ì¸ í•„ìš”
   - RX 580ì€ GFX8 (HSA_OVERRIDE_GFX_VERSION=8.0.3 ì„¤ì • í•„ìˆ˜)

### ë³´ì•ˆ ê³ ë ¤ì‚¬í•­

- ğŸ”’ MCP ì„œë²„ëŠ” ë¡œì»¬ì—ì„œë§Œ ì‹¤í–‰ ê¶Œì¥
- ğŸ›¡ï¸ LiteLLM API í‚¤ëŠ” í™˜ê²½ ë³€ìˆ˜ë¡œ ê´€ë¦¬
- ğŸ” ì›ê²© ì ‘ì† ì‹œ HTTPS + ì¸ì¦ í•„ìˆ˜

---

## ğŸ” íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 1: AMD GPU ì¸ì‹ ì•ˆ ë¨

```bash
# ROCm ì„¤ì¹˜ í™•ì¸
rocm-smi

# ì¶œë ¥ ì—†ìœ¼ë©´ ROCm ì¬ì„¤ì¹˜
sudo apt purge rocm-* -y
sudo apt install rocm-hip-sdk -y

# GFX ë²„ì „ ì˜¤ë²„ë¼ì´ë“œ ì„¤ì •
export HSA_OVERRIDE_GFX_VERSION=8.0.3
```

### ë¬¸ì œ 2: Ollama ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ëŠë¦¼

```bash
# Ollama ë¯¸ëŸ¬ ì„œë²„ ì„¤ì • (í•œêµ­)
export OLLAMA_MIRRORS=https://ollama.kr

# ë˜ëŠ” í”„ë¡ì‹œ ì‚¬ìš©
export http_proxy=http://í”„ë¡ì‹œIP:í¬íŠ¸
export https_proxy=http://í”„ë¡ì‹œIP:í¬íŠ¸
```

### ë¬¸ì œ 3: ë©”ëª¨ë¦¬ ë¶€ì¡± (OOM)

```bash
# ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©
ollama pull deepseek-coder:1.3b

# ë˜ëŠ” ì–‘ìí™” ë²„ì „
ollama pull llama3.1:8b-q4_0
```

### ë¬¸ì œ 4: Claude Code Router ì—°ê²° ì‹¤íŒ¨

```bash
# í™ˆ ì„œë²„ Ollama API ì ‘ê·¼ í…ŒìŠ¤íŠ¸
curl http://í™ˆì„œë²„IP:11434/api/tags

# ë°©í™”ë²½ í™•ì¸
sudo ufw status

# í¬íŠ¸ ì—´ê¸°
sudo ufw allow 11434/tcp
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ

- [Ollama ê³µì‹ ì‚¬ì´íŠ¸](https://ollama.com/)
- [Claude Code Router GitHub](https://github.com/musistudio/claude-code-router)
- [LiteLLM ë¬¸ì„œ](https://docs.litellm.ai/)
- [Model Context Protocol](https://modelcontextprotocol.io/)
- [ROCm for AMD GPUs](https://rocm.docs.amd.com/)

### ì¶”ì²œ ëª¨ë¸

- [DeepSeek-Coder](https://github.com/deepseek-ai/DeepSeek-Coder)
- [CodeLlama](https://ai.meta.com/blog/code-llama-large-language-model-coding/)
- [Llama 3.1](https://ai.meta.com/blog/meta-llama-3-1/)

### ì»¤ë®¤ë‹ˆí‹°

- [Ollama Discord](https://discord.gg/ollama)
- [Claude Code Community](https://github.com/anthropics/claude-code/discussions)
- [r/LocalLLaMA](https://reddit.com/r/LocalLLaMA)

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

1. **ì¦‰ì‹œ ì‹œì‘**: Ollama + DeepSeek-Coder 6.7B ì„¤ì¹˜
2. **ì‹¤í—˜**: ë‹¤ì–‘í•œ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ë° ì„±ëŠ¥ ë¹„êµ
3. **ìµœì í™”**: í•˜ì´ë¸Œë¦¬ë“œ ë¼ìš°íŒ…ìœ¼ë¡œ ë¹„ìš© ì ˆê°
4. **í™•ì¥**: MCP ì„œë²„ë¡œ í™ˆ ì„œë²„ ê¸°ëŠ¥ í†µí•©

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025-12-28
**í…ŒìŠ¤íŠ¸ í™˜ê²½**: AMD Ryzen 5 2400G + RX 580 8GB + Ubuntu 24.04
